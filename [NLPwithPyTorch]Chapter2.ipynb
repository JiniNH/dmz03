{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter2. NLP 기술 빠르게 훑어보기\n",
    "- 자연어처리(NLP) : 정보 추출, 자동 음성 인식, 기계번역, 감성 분석, 질의 응답, 요약 같은 실용적인 문제를 해결하는 방법을 개발하는 것\n",
    "- 전산 언어학(Computational Linguistics) : 컴퓨터 언어의 특징을 이해하는 방법을 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 말뭉치, 토큰, 타입\n",
    "- 모든 NLP 작업은 말뭉치 corpus라 부르는 텍스트 데이터로 시작\n",
    "- 말뭉치는 일반적으로 원시 텍스트와 이 텍스트에 연관된 메타데이터를 포함\n",
    "- 원시 텍스트는 일반적으로 문자를 토큰token 이라는 연속된 단어로 묶었을 때 유용\n",
    "- 영어에서 토큰은 공백문자나 구두점으로 구분되는 단어와 숫자에 해당\n",
    "- 머신러닝 분야에서 메터데이터가 붙은 텍스트를 샘플sample 또는 데이터 포인트 data point라고 부르며 샘플의 모음인 말뭉치는 데이터셋dataset 이라고 한다\n",
    "- 토큰화tokenization는 텍스트를 토큰으로 나누는 과정. 오픈소스 NLP 패키지는 기본적인 토큰화를 제공\n",
    "- 타입type은 말뭉치에 등장하는 고유한 토큰. 말뭉치에 있는 모든 타입의 집합이 어휘사전 또는 어휘lexicon\n",
    "- 단어는 내용어content words와 불용어stopword로 구분. 관사와 전치사 같은 불용어는 대부분 내용어를 보충하는 문법적인 용도로 사용\n",
    "- 특성공학 feature engineering : 언어학을 이해하고 NLP 문제 해결에 적용하는 과정을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 토큰화\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':', '-', '_']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-_\"\n",
    "tokenizer= TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 유니그램, 바이그램, 트라이그램, ..., n-그램\n",
    "- n-gram은 텍스트에 있는 고정 길이(n)의 연속된 토큰 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mary', ',', \"n't\"],\n",
       " [',', \"n't\", 'slap'],\n",
       " [\"n't\", 'slap', 'green'],\n",
       " ['slap', 'green', 'witch'],\n",
       " ['green', 'witch', '.']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트에서 n-그램 만들기\n",
    "def n_grams(text, n):\n",
    "    '''\n",
    "    takes tokens or text, returns a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = [\"mary\", \",\", \"n't\", \"slap\", \"green\", \"witch\", \".\"]\n",
    "n_grams(cleaned, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 표제어와 어간\n",
    "- 표제어lemma는 단어의 기본형\n",
    "- 표제어 추출lemmatization : 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법\n",
    "- 어간 추출stemming : 표제어 추출 대신에 사용하는 축소 기법으로 수동으로 만든 규칙을 사용해 단어의 끝을 잘라 어간stem이라는 공통 형태로 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"he was running late\")\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> wa\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "# 어간 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "doc = \"he was running late\"\n",
    "token_doc = word_tokenize(doc)\n",
    "\n",
    "for token in token_doc:\n",
    "    print('{} --> {}'.format(token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문장과 문서 분류하기\n",
    "- 문서 분류 작업은 NLP 분야의 초기 애플리케이션 중 하나\n",
    "- TF와 TF-IDF표현이 문서나 문장 같은 긴 텍스트 뭉치를 분류하는데 유용\n",
    "- 토픽 레이블 할당, 리뷰의 감성 예측, 스팸 이메일 필터링, 언어 식별, 이메일 분류 같은 작업은 지도 학습 기반의 문서 분류 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 단어 분류하기 : 품사 태깅\n",
    "- 문서에 레이블을 할당하는 개념을 단어나 토큰으로 확장\n",
    "- 단어 분류 작업의 예로는 품사part-of-speech(POS) 태킹tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\spacy\\util.py:1676: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - PROPN\n",
      "slapped - VERB\n",
      "the - DET\n",
      "green - ADJ\n",
      "witch - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 청크 나누기와 개체명 인식\n",
    "- 종종 연속된 여러 토큰으로 구분되는 텍스트 구에 레이블을 할당\n",
    "- 예를들어 명사구(NP)와 동사구(VP)를 구별하는 것과 같은 작업을 청크 나누기chunking 또는 부분 구문 분석shallow parsing라고 한다\n",
    "- 부분 구문 분석의 목적은 명사, 동사, 형용사 같은 문법 요소로 구성된 고차원의 단어를 유도해 내는 것\n",
    "- 부분 구문 분석 모델 훈련에 사용할 데이터가 없다면 품사 태깅에 정규식을 적용해 부분 구문 분석을 근사\n",
    "- 또 다른 유용한 단위는 개체명named entity. 개체명은 사람, 장소, 회사, 약 이름과 같은 실제 세상의 개념을 의미하는 문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "# 명사구(NP) 부분 구문 분석\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('{} - {}'.format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 문장 구조\n",
    "- 구문 분석parsing : 구 단위를 식별하는 부분 구문 분석과 달리 구 사이의 관계를 파악하는 작업\n",
    "- 구분 분석 트리parse tree는 문장안의 문법 요소가 계층적으로 어떻게 관련되는지를 보여준다\n",
    "    - 구성 구문 분석 constituent parsing\n",
    "    - 의존 구문 분석 dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 단어 의미와 의미론\n",
    "- 단어에는 의미가 하나 이상 있고 단어가 나타내는 각각의 뜻을 단어의 의미sense라고 한다\n",
    "- 단어 의미는 문맥으로 결정될 수 있다\n",
    "- 텍스트에서 단어 의미를 자동으로 찾는 일은 실제로 NLP에 적용된 첫 번째 준지도 학습 semi-supervised learning 이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e7d24ecb1dc1117e8330ee9e498b85da846e4ffb1348d12e4a7f0695b68a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
