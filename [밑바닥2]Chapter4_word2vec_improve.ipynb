{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter4. word2vec 속도개선\n",
    "### 1. word2vec 개선1\n",
    "- 거대한 말뭉치를 다루게 되면 몇가지 문제가 발생\n",
    "- 입력층과 출력층에 수많은 뉴런이 존재하고 중간 계산에 많은 시간이 소요\n",
    "- 정확히는 다음의 두 계산이 병목\n",
    "\n",
    "\n",
    "\n",
    "    1. 입력층의 원핫 표현과 가중치 행렬 W_in 의 곱 계산. 단어를 원 핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커지고  \n",
    "    가중치 행렬 W_in을 곱하는데 상당히 많은 양의 자원을 사용\n",
    "        - Embedding 계층을 도입하여 해결\n",
    "    <br><br>\n",
    "\n",
    "    2. 은닉층과 가중치 행렬 W_out 의 곱 및 Softmax 계층의 계산. 은닉층과 가중치 행렬 W_out의 곱만 해도 계산량이 상당하다.  \n",
    "    그리고 Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가하는 문제가 있다\n",
    "        - 네가티브 샘플링이라는 새로운 손실합수를 도입해 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Embedding 계층\n",
    "- 원핫 표현과 MatMul 계층의 가중행렬을 곱하는 것이 결과적으로 수행하는 일은 단지 행렬의 특정 행을 추출하는 것이다\n",
    "- 원핫 표현의로의 변환 과 MatMul 계층의 행렬 곱 계산 대신 가중치 매개변수로부터 단어 ID에 해당하는 행(벡터)를 추출하는 계층을 만들어서 해결\n",
    "- 이 계층을 Embedding 계층이라 한다. (word embedding 에서 유래)\n",
    "- 자연어차리분야에서 단어 밀집벡터 표현 : word embedding or distributed representation (분산표현)\n",
    "    - 통계기반기법 : distributional representation\n",
    "    - 신경망을 사용한 추론 기반 기법 : distributed representation\n",
    "    - 둘다 분산 표현이지만 단어가 미묘하게 다르다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Embedding 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 특정 행 명시\n",
    "import numpy as np\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 16, 17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([1,0,3,0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 계층 구현\n",
    "import cupyx\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    \n",
    "    # forward()\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    # backword()\n",
    "    # 안좋은예, idx 원소 중복 시 다음 값이 이전값을 닾어씀\n",
    "    # def backward(self, dout):\n",
    "    #     dW, = self.grads\n",
    "    #     dW[...] = 0   # dW 0 설정 X, dW 형상 유지하며 그 원소를 0으로 덮어쓴다\n",
    "    #     dW[self.idx] = dout  \n",
    "    #     return None\n",
    "\n",
    "    # 할당이 아닌 더하기\n",
    "    # (행렬곱을 취하는 식 기준으로 역전파 시행 과정 참고)\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "\n",
    "        # 계산효율상 for 문보다는 numpy 메서드 사용\n",
    "        # for i, word_id in enumerate(self.idx):\n",
    "        #     dW[word_id] += dout[i]\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        \n",
    "        # # cupy 버전에 따른 오류 해결을 위해\n",
    "        # if GPU:\n",
    "        #     cupyx.scatter_add(dW, self.idx, dout)\n",
    "        # else:\n",
    "        #     np.add.at(dW, self.idx, dout)           \n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. word2vec 개선2\n",
    "- 은닉층 이후의 처리(행렬 곱과 Softmax 계층의 계산). 네거티브 샘플링 기법 사용 병목 해소\n",
    "- Softmax 대신 네거티브 샘플링을 이용하면 어휘가 ㅁ낳아져도 계싼량을 낮은 수준에서 일정하게 억제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 은닉층 이후 계산의 문제점\n",
    "- 은닉층의 뉴런과 가중치 행렬(W_out)의 곱, Softmax 계층의 계산\n",
    "- 거대한 행렬을 곱하는 문제로 많은 시간과 메모리 투입 - 가벼운 계산이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 다중 분류에서 이진 분류로\n",
    "- 다중분류를 이진분류로 근사하는 것이 네거티브 샘플링을 이해하는데 중요한 포인트\n",
    "- 예를 들어 'you'와 'goodbye'를 주면 정답인 'say' 확률이 높아지도록 학습 시켰지만, \n",
    "- 'you'와 'goodbye'일 때 타깃단어는 'say' 입니까 라는 Y/N 질문으로 변경\n",
    "- 출력 측의 가중치 W_out에서는 각 단어 ID의 단어 벡터가 각각의 열로 저장\n",
    "- 단어 벡터를 추출하여 그 벡터와 은닉층 뉴적과의 내적을 구하고 이 값이 최종점수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "- 이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고 손실을 구할 때는 손실 함수로 교차엔트로피 오차를 사용\n",
    "- $Sigmoid\\; function\\;\\; y=\\cfrac{1}{1+exp(-x)}$<br>\n",
    "<br>$Loss\\;funtion\\;\\;L=-(tlogy + (1-t)log(-y))$<br>\n",
    "<br>$Yes\\;t=1\\;:\\;-tlogy,\\;\\;\\;\\;No\\;t=0\\;:\\;-log(1-y)$<br>\n",
    "<br>$forward\\;:\\; x \\to sigmoid \\to y \\to CrossEntropy \\to Loss$<br>\n",
    "<br>$backward\\;:\\; \\frac{\\partial L}{\\partial x} \\gets sigmoid \\gets \\frac{\\partial L}{\\partial y} \\gets CrossEntropy \\gets 1$<br>\n",
    "<br>$\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$<br>\n",
    "<br>$\\frac{\\partial L}{\\partial y}=-\\frac{t}{y}+\\frac{1-t}{1-y}=\\frac{y-t}{y(1-y)}$<br>\n",
    "<br>$\\frac{\\partial y}{\\partial x}=y(1-y)$<br>\n",
    "<br>$\\frac{\\partial L}{\\partial x}=y-t$<br><br>\n",
    "- 역전파 y-t는 확률과 정답의 차이. 오차가 앞 계층으로 흘러가 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 다중 분류에서 이진 분류로 구현\n",
    "- 은닉층 뉴런 h와 철력 측의 가중치 W_out에서 단어 벡터와의 내적을 계산하고 그 출력을 Sigmoid with Loss 계층에 입력해 최종 손실을 얻는다\n",
    "- 이를 위해 Embedding 계층과 dot 연산(내적)의 처리를 합친 Embedding Dot 계층을 도입\n",
    "- 은닉층 뉴런 h는 Embedding Dot 계층을 거쳐 Sigmoid with Loss 계층을 통과. 은닉층 이후 처리가 간단해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Dot 계층의 구현\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)  # numpy array idx\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.chace\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 네거티브 샘플링\n",
    "- 긍정적 예(정답)을 입력하면 Sigmoid 계층 출력이 1에 가깝고, 부정적 예(오답)를 입력했을 때의 출력은 0에 가깝게 만드는 가중치가 필요하다\n",
    "- 다중 분류 문제를 이진 분류로 다루려면 정답과 오답 각각에 대해 바르게 분류할 수 있어야 한다\n",
    "- 그렇다고 모든 부정적 예를 대상으로 하여 이진 분류를 학습하면 어휘 수가 늘어나 감당할수 없다\n",
    "- 근사적인 해법으로, 적은 수의 부정적 예를 샘플링해 사용한다. 이것이 바로 네거티브 샘플링 기법이다\n",
    "- 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구하고 동시에 부정적 예를 몇 개 샘플링하여 손실을 구하고 각각의 데이터의 손실을 더한 값을 최종 손실로 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 네거티브 샘플링의 샘플링 기법\n",
    "- 단순 무작위 샘플링보다 말뭉치의 통계 데이터를 기초로 샘플링하는 방법이 더 좋다\n",
    "- 말뭉치에서 각 단어의 출연 횟수를 구해 확률분포로 나타내고, 이 확률분포대로 단어를 샘플링\n",
    "- 네거티브 샘플링은 부정적 예를 많이 다루는 것이 좋지만 계산량 문제때문에 적은수로 한정. 이때 우연히도 희소한 단어가 선택되면 결과도 나빠진다\n",
    "- 즉 흔한 단어를 잘 처리하는 것이 좋은 결과로 이어질 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률분포에 따라 샘플링하는 예\n",
    "import numpy as np\n",
    "\n",
    "# 0~9 숫자 중 무작위 샘플링\n",
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words에서 하나만 무작위로 샘플링\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I', '.', '.', '.', 'goodbye'], dtype='<U7')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5개만 무작위로 샘플링(중복 있음)\n",
    "np.random.choice(words, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', 'hello', 'say', 'I', 'goodbye'], dtype='<U7')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5개만 무작위로 샘플링(중복 없음)\n",
    "np.random.choice(words, size=5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['you', 'you', 'I', 'hello', 'you'], dtype='<U7')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률분포에 따라 샘플링\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p=p, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64196878, 0.33150408, 0.02652714])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [0 2]\n",
      " [2 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch04')\n",
    "\n",
    "from negative_sampling_layer import UnigramSampler\n",
    "\n",
    "# UnigramSampler 클래스\n",
    "corpus = np.array([0,1,2,3,4,1,2,3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 네거티브 샘플링 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    # 순전파    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # 역전파\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 개선판 word2vec 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 CBOW 모델 구현\n",
    "- SimpleCBOW 클래스 개선 CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch04')\n",
    "import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)     # Embedding 계층 사용\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 CBOW 모델 학습 코드\n",
    "- CBOW 모델의 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-71f4b644c47e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# 학습 시작\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\14.밑바닥부터시작하는딥러닝\\2\\common\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# 기울기 구해 매개변수 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 공유된 가중치를 하나로 모음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/Python/14.밑바닥부터시작하는딥러닝/2/ch04\\cbow.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, contexts, target)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\14.밑바닥부터시작하는딥러닝\\2\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__array__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/common')\n",
    "\n",
    "import numpy as np\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ===============================================\n",
    "# config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from skip_gram import SkipGram\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e7d24ecb1dc1117e8330ee9e498b85da846e4ffb1348d12e4a7f0695b68a9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
