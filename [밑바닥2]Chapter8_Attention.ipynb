{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 어텐션의 구조\n",
    "- seq2seq를 한층 더 강력하게 하는 어텐션 메커니즘 덕분에 seq2seq는 필요한 정보에만 주목할 수 있게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 seq2seq의 문제점\n",
    "- seq2seq에서는 Encoder가 시계열 데이터를 인코딩하고 인코딩 된 정보를 Decoder로 전달\n",
    "- 이때 Encoder의 출력은 고정 길이의 벡터. 고정길이는 항상 같은 길이의 벡터를 밀어 넣는 큰 문제가 잠재\n",
    "- 필요한 정보가 벡터에 다 담기지 못하게 된다. Encoder와 Decoder 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Encoder 개선\n",
    "- LSTM 계층의 마지막 은닉 상태만을 Decoder에 전달\n",
    "- Encoder 출력의 길이는 입력 문장의 길이에 따라 바꿔주는 것이 Encoder의 개선 포인트. 구체적으로 LSTM 계층의 은닉 상태 벡터를 모두 이용 하는 것이다\n",
    "- 은닉 상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻고 Encoder는 하나의 고정 길이 벡터라는 제약으로부터 해방 된다\n",
    "- 많은 딥러닝 프레임워크에서는 RNN 계층(혹은 LSTM, GRU 계층 등)을 초기화할 때, 모든 시각의 은닉 상태 벡터 반환과 마지막 은닉 상태 벡터만 반환 중 선택 가능\n",
    "- Encoder의 은닉 상태를 모든 시각만큼 꺼냄으로써 입력 문장의 길이에 비례한 정보를 인코딩할 수 있게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Decoder 개선 ①\n",
    "- Encoder는 각 단어에 대응하는 LSTM 계층의 은닉 상태 벡터를 hs로 모아 출력하고 hs가 Decoder에 전달되어 시계열 변환이 이루어 진다\n",
    "- 입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가 라는 대응관계를 seq2seq에 학습\n",
    "- 단어의 대응관계를 나타내는 정보를 얼라인먼트(alignment)라 하는데 지금까지 주로 사람의 수작업으로 만듦\n",
    "- 도착어 단어와 대응 관계에 있는 출발어 단어의 정보를 골라내는 것. 그리고 그 정보를 이용하여 번역 수행\n",
    "- 필요한 정보에만 주목하하여 그 정보로부터 시계열 반환을 수행하는 것이 목표\n",
    "- Encoder로 부터 받은 hs, 시각별 LSTM계층의 은닉상태 2가지 입력을 받아 어떤 계산을 수행(얼라인먼트 추출)\n",
    "- 대응벡터를 선택(미분가능하게 모든 벡터를 선택하여 가중치 계산). 단어의 중요도 가중치 a와 각 단어의 벡터 hs로부터 가중합을 구하여 맥락 벡터 c를 구한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# 가중합 구현\n",
    "# 시계열 길이 T=5, 은닉 상태 벡터 원소수 H=4로 가중합\n",
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "# 브로드캐스트 이용도 가능하나 눈에 보이지 않으므로 형상화\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 처리용 가중합 구현\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "# ar = a.reshape(N, T, 1) # 브로드캐스트 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Sum 계층\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum의 역전파\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)    # repeat의 역전파\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Decoder 개선 ②\n",
    "\n",
    "- 가중치 a를 구하는 방법 살펴보기\n",
    "- 지금의 목표는 Decoder의 LSTM 계층의 은닉 상태 벡터 h가 hs의 각 단어 벡터와 얼마나 비슷한가를 내적을 이용해 수치로 나타내는 것. 두 벡터의 유사도를 표현하는 척도\n",
    "- h와 hs의 각 단어 벡터와의 유사도 결과 s. softmax를 통한 정규화하여 각 단어의 가중치 a 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch08')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "# hr = h.reshape(N, 1, H)   # 브로드캐스트 사용하는 경우\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttentionWeight 클래스 구현\n",
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch08')\n",
    "from common.np import * #import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hr, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Decoder 개선 ③\n",
    "- Attention Weight 계층과 Weight Sum 계층을 구현. 이 두계층을 하나로 결합하여 Attention 계층\n",
    "- Attention Weight 계층은 Encoder가 출력하는 각 단어의 벡터 hs에 주목하여 해당 단어의 가중치 a를 구한다\n",
    "- Weight Sum 계층이 a와 hs의 가중합을 구하고, 그 결과를 맥락 벡터 c로\n",
    "- Attention 계층을 LSTM 계층과 Affine 계층 사이에 삽입. Attention 계층에는 Encoder의 출력인 hs가 입력\n",
    "- 또, 여기에 LSTM 계층의 은닉 상태 벡터를 Affine 계층에 입력\n",
    "- 마지막으로 시계열 방향으로 펼쳐진 다수의 Attention 계층을 Time Attention 계층으로 모아 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 계층 구현\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads =[], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Attention 계층 구현\n",
    "import outcome\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Encoder 구현\n",
    "- LSTM 계층의 마지막 은닉 상태 벡터만 반화했던 것을 모든 은닉 상태 반환으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 상속하여 구현\n",
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch08')\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Decoder 구현\n",
    "- forward, backward 메서드에 새로운 단어열(혹은 문자열) 생성 generate 메서드 추가\n",
    "- forward() 메서드에서 Time Attention 계층의 출력과 LSTM 계층의 출력을 연결 (np.concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()  # Attention 레이어 \n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)  # context vector\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # context_vector & lstm h_t\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 seq2seq 구현\n",
    "- Seq2seq 클래스 상속, AttentionEncoder, AttentionDecoder 클래스 사용 AttentionSeq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSofmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 날짜 형식 변환 문제\n",
    "- 영어권에서 사용되는 다양한 날짜 형식을 표준 형식으로 변환하는 것이 목표\n",
    "- ex) september 27, 1994 → 1994-09-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 7[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 15[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 23[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 31[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 40[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 50[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 58[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 66[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 74[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 82[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 89[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 97[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 105[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 113[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 121[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 128[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 137[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 8[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 16[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 24[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 32[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 40[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 48[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 56[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 64[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 72[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 81[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 89[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 97[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 105[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 113[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 122[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 130[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 138[s] | 손실 0.47\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 8[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 16[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 24[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 32[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 40[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 48[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 56[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 64[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 72[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 80[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 88[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 96[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 103[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 111[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 119[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 127[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 135[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 8[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 17[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 25[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 33[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 41[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 50[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 58[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 107[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 115[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 141[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 17[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 25[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 65[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 121[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 137[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 51[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 100[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 108[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 116[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 133[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 141[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 65[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 106[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 122[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 130[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 139[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 95[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 134[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 101[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 117[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 133[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 117[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 133[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 11 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 181 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 201 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 221 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 241 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 261 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 301 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 321 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 11 |  반복 341 / 351 | 시간 134[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 12 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 61 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 181 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 201 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 221 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 241 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 261 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 301 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 321 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 12 |  반복 341 / 351 | 시간 133[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 13 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 181 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 201 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 221 / 351 | 시간 89[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 241 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 261 / 351 | 시간 105[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 281 / 351 | 시간 113[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 301 / 351 | 시간 121[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 321 / 351 | 시간 130[s] | 손실 0.00\n",
      "| 에폭 13 |  반복 341 / 351 | 시간 138[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 14 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 41 / 351 | 시간 17[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 61 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 81 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 101 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 121 / 351 | 시간 50[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 141 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 161 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 181 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 201 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 221 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 241 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 261 / 351 | 시간 108[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 281 / 351 | 시간 116[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 301 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 321 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 14 |  반복 341 / 351 | 시간 140[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 15 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 101 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 121 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 141 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 161 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 181 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 201 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 221 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 241 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 261 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 281 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 301 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 321 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 15 |  반복 341 / 351 | 시간 136[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 16 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 101 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 121 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 141 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 161 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 181 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 201 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 221 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 241 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 261 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 281 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 301 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 321 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 16 |  반복 341 / 351 | 시간 136[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 17 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 181 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 201 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 221 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 241 / 351 | 시간 95[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 261 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 301 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 321 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 17 |  반복 341 / 351 | 시간 134[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 18 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 181 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 201 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 221 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 241 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 261 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 281 / 351 | 시간 110[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 301 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 321 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 18 |  반복 341 / 351 | 시간 134[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 19 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 81 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 101 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 121 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 161 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 181 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 201 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 221 / 351 | 시간 87[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 241 / 351 | 시간 95[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 261 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 281 / 351 | 시간 111[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 301 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 321 / 351 | 시간 127[s] | 손실 0.00\n",
      "| 에폭 19 |  반복 341 / 351 | 시간 135[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n",
      "| 에폭 20 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 21 / 351 | 시간 8[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 41 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 61 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 81 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 101 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 121 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 141 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 161 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 181 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 201 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 221 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 241 / 351 | 시간 98[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 261 / 351 | 시간 106[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 281 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 301 / 351 | 시간 122[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 321 / 351 | 시간 131[s] | 손실 0.00\n",
      "| 에폭 20 |  반복 341 / 351 | 시간 139[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc %.3%%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch08')\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch07')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 20\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 어텐션 시각화\n",
    "- 어텐션이 시계열 변환을 수행할 때, 어느 원소에 주의를 기울이는지를 살펴보려는 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Python/14.밑바닥부터시작하는딥러닝/2/ch08')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 어텐션에 관한 남은 이야기\n",
    "- 어텐션에 관련하여 다루지 못한 주제 몇 가지 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 양방향 RNN\n",
    "- seq2seq의 Encoder에 초점\n",
    "- LSTM의 각 시각의 은닉상태 벡터는 hs로 모아진다\n",
    "- 여기서 주목할 것은 글을 왼쪽에서 오른쪽으로 읽는다는 점이다. 따라서 '나', '는', '고양이', '로소', '이다' 예시에서 '고양이'에 대응하는 벡터에 '나', '는', '고양이' 까지 총 세 단어의 정보가 인코딩된다\n",
    "- 전체적인 균형을 생각해 단어의 주변 정보를 균형있게 담고 싶을 것이다. 그래서 LSTM을 양방향으로 처리하는 방법이 양방향 LSTM(양방향RNN) 기술이다\n",
    "- 기존 LSTM 계층에 역방향 처리 LSTM 계층을 추가. 각 시각에서 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리(연결 외에 합이나 평균 등의 방법도 생각 할 수 있다)\n",
    "- 2개의 LSTM 계층을 사용하여 각각의 계층에 주는 단어의 순서를 조정(왼쪽에서 오른쪽 + 오른쪽에서 왼쪽)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Attention 계층 사용 방법\n",
    "- 앞에서 Attention 계층을 LSTM 계층과 Affine 계층 사이에 삽입\n",
    "- Attention 계층의 출력(맥락 벡터)이 다음 시각의 LSTM 계층에 입력되도록 연결하면 LSTM 계층이 맥락 벡터의 정보를 이용할 수 있다\n",
    "- Attention 계층의 위치를 달리하는 게 최종 정확도에 주는 영향은 실제 데이터로 검증해봐야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 seq2seq 심층화와 skip 연결\n",
    "- 기계 번역 등 현실에서의 애플리케이션들은 풀어야 할 문제가 훨씬 북잡하고 더 높은 표현력이 요구된다\n",
    "- 먼저 생각해야 할 것은 RNN계층(LSTM 계층)의 층을 깊게 쌓는 방법이다\n",
    "- 일반적으로 Encoder와 Decoder에서는 같은 층수의 LSTM 계층을 이용한다\n",
    "- 층을 깊게 할 때 사용되는 중요한 기법 중 skip connection 이 있다(잔차 연결 residual connection 또는 숏컷 short-cut 이라고도 한다). 계층을 넘어 선을 연결하는 단순한 기법이다\n",
    "- skip 연결은 계층을 건너뛰는 연결로 이때 skip연결의 접속부에서는 2개의 출력이 더해진다. 이 덧셈이 핵심으로 덧셈은 역전파 시 기울기를 드래도 흘려보내므로, skip 연결의 기울기가 아무런 영향을 받지 않고 모든 계층으로 흐르기 떄문에 층이 깊어져도 기울기 소실(폭팔)되지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 어텐션 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 구글 신경망 기계 번역 (GNMT, Google Nueral Machine Translation)\n",
    "- 기계 번역의 흐름 : 규칙 기반 → 용례 기반 → 통계 기반 → 신경망 기계 번역(Neural Machgine Translation)\n",
    "- 신경망 기계 번역은 기존 통계 기반 번역과 대비되는 형태로 사용, 최근에는 seq2seq 사용한 기계 번역의 총칭\n",
    "- GNMT도 어텐션을 갖춘 seq2seq와 마찬가지로 Encoder와 Decoder, Attention으로 구성\n",
    "- 번역의 정확도를 높이기 위해 LSTM 계층의 다층화, 양방향 LSTM, skip 연경 등 개선 추가\n",
    "- 학습 시간 단축하기 위해 다수의 GPU로 분산 학습을 수행\n",
    "- 이상의 아키텍처적인 연구 외에도, 낮은 빈도의 단어 처리나 추론 고속화를 위한 양자화 등 다양한 연구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 트랜스포머 Transformer\n",
    "- RNN은 이전 시각에 계산한 결과를 이용하여 순서대로 계싼. 따라서 시간 방향으로 병렬 계산하기란 불가능\n",
    "- 이 점은 딥러닝 학습이 GPU를 사용한 병렬 계싼 환경에서 이뤄진다는 점을 생각하면 큰 병목 문제이다\n",
    "- RNN을 없애거나 병렬 계산할 수 있는 연구가 활발 - Attention is all you need 논문에서 제안한 트랜스포머\n",
    "- 트랜스포머는 어텐션으로 구성되는데, 그중 셀프어텐션 Self-Attention 기술을 이용하는게 핵심\n",
    "- 하나의 시계열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되는지 살펴보자는 취지의 Self-Attention\n",
    "- Time Attention 계층에는 서로 다른 두 시계열 데이터가 입력되는 반면, Self-Attention은 두 입련이 모두 하나의 시계열 데이터로 부터 나온다. 이렇게 하면 하나의 시계열 데이터 내에서의 원소간 대응 관계가 구해진다\n",
    "- 트랜스포머에서는 RNN 대신 Attention을 사용. 위의 그림을 보면 Encoder와 Decoder 모두에서 셀프어텐션을 사용한다. 또한 Feed Forward 계층은 피드포워드 신경망을 정확하게는 은닉층이 1개이고 활성화 함수로 ReLU를 이용한 완전 연결계층 신경망을 이용한다\n",
    "- Nx는 해당 계층이 N겹 쌓였다는 뜻\n",
    "- 트랜스포머를 이용하면 계산량을 줄이고 GPU를 이용한 병렬 계산의 혜택도 더 많이 누릴수 있고, 그 결과 GNMT보다 학습 시간을 큰 폭으로 줄이고 번역 품질도 상당 폭 끌어올릴 수 있었다\n",
    "\n",
    "\n",
    "![](2022-07-16-08-30-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 뉴럴 튜링 머신 (Neural Turing Machine, NMT)\n",
    "- 어텐션을 갖춘 seq2seq에서는 Encoder가 입력 문장을 인코딩한다. 그리고 인코딩된 정보를 어텐션을 통해 Decoder가 이용한다. 즉, Encoder가 필요한 정보를 메모리에 쓰고, Decoder는 그 메모리로부터 필요한 정보를 읽어 들인다\n",
    "- 컴퓨터의 메모리 조작을 신경망에서도 재현. RNN의 외부에 정보 저장용 메모리 기능을 배치하고, 어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법이다. 그중 유명한 연구가 뉴럴 튜링 머신이다\n",
    "- NTM은 외부 메모리를 읽고 쓰면서 시계열 데이터를 처리. 이러한 메모리 조작을 미분가능한 계산으로 구축\n",
    "- LSTM 계층이 컨트롤러가 되어 NTM의 주된 처리를 수행. 각 시각에서 LSTM 계층의 은닉 상태를 Write Head 계층이 받아서 필요한 정보를 메모리에 쓴다음 Read Head 계층이 메모리로부터 중요한 정보를 읽어 들여 다음 시각의 LSTM 계층으로 전달\n",
    "- NTM은 컴퓨터의 메모리 조작을 모방하기 위해서 2개의 어텐션을 이용\n",
    "- 콘텐츠 기반 어텐션 : 지금까지 본 어텐션과 같고, 입력으로 주어진 어느 벡트와 비슷한 벡터를 메모리로부터 찾아내는 용도로 이용\n",
    "- 위치 기반 어텐션 : 이전 시각에서 주목한 메모리의 위치(=메모리의 각 위치에 대한 가중치)를 기준으로 그 전후로 이동(시프트)하는 용도로 사용\n",
    "- 외부 메모리를 자유롭게 이용하게 됨으로써 NTM은 seq2seq 만으로 풀리지 않던 긴 시계열을 기억하는 문제와 정렬 등의 문제를 해결\n",
    "- 외부 메모리를 사용함으로써 알고리즘을 학습하는 능력을 얻는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 정리\n",
    "- 번역이나 음성 인식 등, 한 시계열 데이터를 다른 시계열 데이터로 변환하는 작업에서는 시계열 데이터 사이의 대응관계가 존재하는 경우가 많다\n",
    "- 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다\n",
    "- 어텐션에는 벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 된다\n",
    "- 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습할 수 있다\n",
    "- 어텐션이 산출하는 가중치를 시각화하면 입출력의 대응관계를 볼 수 있다\n",
    "- 외부 메모리를 활용한 신경망 확장 연구 예에서는 메모리를 읽고 쓰는데 어텐션을 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e7d24ecb1dc1117e8330ee9e498b85da846e4ffb1348d12e4a7f0695b68a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
